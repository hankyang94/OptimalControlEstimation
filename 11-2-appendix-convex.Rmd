# Convex Analysis and Optimization {#appconvex}

## Theory {#appconvex-theory}

### Sets

Convex set is one of the most important concepts in convex optimization. Checking convexity of sets is crucial to determining whether a problem is a convex problem. Here we will present some definitions of some set notations in convex optimization.

::: {.definitionbox}
::: {.definition #affineset name="Affine set"}
  A set $C\subset \mathbb{R}^n$ is affine if the line through any two distinct points in $C$ lies in $C$, i.e., if for any $x_1,x_2 \in C$ and any $\theta \in \mathbb{R}$, we have $\theta x_1 + (1-\theta)x_2 \in C$.
:::
:::

::: {.definitionbox}
::: {.definition #convexset name="Convex set"}
  A set $C\subset \mathbb{R}^n$ is convex if the line segment between any two distinct points in $C$ lies in $C$, i.e., if for any $x_1,x_2 \in C$ and any $\theta \in [0,1]$, we have $\theta x_1 + (1-\theta)x_2 \in C$.
:::
:::

::: {.definitionbox}
::: {.definition #cone name="Cone"}
  A set $C\subset \mathbb{R}^n$ is a cone if for any $x\in C$ and any $\theta\geq 0$, we have $\theta x \in C$.
:::
:::

::: {.definitionbox}
::: {.definition #convexcone name="Convex Cone"}
  A set $C\subset \mathbb{R}^n$ is a convex cone if $C$ is convex and a cone.
:::
:::

Below are some important examples of convex sets:

::: {.definitionbox}
::: {.definition #hyperplane name="Hyperplane"}
  A hyperplane is a set of the form $$\{x|a^Tx = b\}$$
:::
:::

::: {.definitionbox}
::: {.definition #halfspaces name="Halfspaces"}
  A (closed) halfspace is a set of the form $$\{x|a^Tx \leq b\}$$
:::
:::

::: {.definitionbox}
::: {.definition #balls name="Balls"}
  A ball is a set of the form $$B(x,r) = \{y|\|y-x\|_2 \leq r\} = \{x+ru|\|u\|_2\leq 1\}$$
  where $r >0$.
:::
:::

::: {.definitionbox}
::: {.definition #ellipsoids name="Ellipsoids"}
  A ellipsoid is a set of the form $$\mathcal{E} = \{y|(y-x)^TP^{-1}(y-x)\leq 1\}$$
  where $P$ is symmetric and positive definite.
:::
:::

::: {.definitionbox}
::: {.definition #polyhedra name="Polyhedra"}
  A polyhedra is defined as the solution set of a finite number of linear equalities
and inequalities: $$\mathcal{P} = \{x|a_j^Tx\leq b_j, j=1,...,m, c_k^Tx=d_k,k=1,...,p\}$$
:::
:::

::: {.definitionbox}
::: {.definition #normball name="Norm ball"}
  A norm ball $B$ of radius $r$ and a center $x_c$ associated with the norm $\|\cdot\|$ is defined as: $$B = \{x|\|x-x_c\|\leq r\}$$
:::
:::

::: {.definitionbox}
::: {.definition #normcone name="Norm cone"}
  A norm cone $C$ associated with the norm $\|\cdot\|$ is defined as: $$C = \{(x,t)|\|x\|\leq t\}\subset \mathbb{R}^{n+1}$$
:::
:::

Simplexes are important family of polyhedra. Suppose the $k+1$ points $v_0,...,v_k\in \mathbb{R}^n$ are affinely independent, which means $v_1-v_0,...,v_k-v_0$ are linearly independent.

::: {.definitionbox}
::: {.definition #simplex name="Simplex"}
  A simplex $C$ defined by points $v_0,...,v_k$ is: $$C = \textbf{conv}\{v_0,...,v_k\} = \{\theta_0v_0 + ... \theta_kv_k|\theta \succeq 0, \textbf{1}^T\theta = 1\}$$
:::
:::

Extremely important examples of convex sets are positive semidefinite cones:

::: {.definitionbox}
::: {.definition #symmetricmatrices name="Symmetric,positive semidefinite,positive definite matrices"}
  1. Symmetric matrices: $\textbf{S}^n = \{X\in\mathbb{R}^{n\times n}| X=X^T\}$
  2. Symmetric Positive Semidefinite matrices: $\textbf{S}_+^n = \{X\in\textbf{S}^n| X\succeq0\}$
  3. Symmetric Positive definite matrices: $\textbf{S}_{++}^n = \{X\in\textbf{S}^n| X\succ0\}$
:::
:::

In most scenarios, the set we encounter is more complicated. In general it is extermely hard to determine whether a set in convex or not. But if the set is 'generated' by some convex sets, we can easily determine its convexity. So let's focus on operations that preserve convexity:

::: {.theorembox}
::: {.proposition #operationpreserveconvexity}
  Assume $S$ is convex, $S_\alpha,\alpha\in\mathcal{A}$ is a family of convex sets. Following operations on convex sets will preserve convexity:
  
  1. Intersection: $\bigcap_{\alpha\in\mathcal{A}}S_\alpha$ is convex.
  
  2. Image under affine function: A function $f:\mathbb{R}^n\to\mathbb{R}^m$ is affine if it has the form $f(x) = Ax+b$. The image of $S$ under affine function $f$ is convex. I.e. $f(S) = \{f(x)|x\in S\}$ is convex
  
  3. Image under perspective function: We define the perspective function $P:\mathbb{R}^{n+1}$, with domain $\textbf{dom}P = \mathbb{R}^n\times \mathbb{R}_{++}$(where $\mathbb{R}_{++}=\{x\in \mathbb{R}|x>0\}$) as $P(z,t) = z/t$. The image of $S$ under perspective function is convex.
  
  4. Image under linear-fractional function: We define linear fractional function $f:\mathbb{R}^n\to\mathbb{R}^m$ as:$f(x) = (Ax+b)/(c^Tx+d)$ with $\textbf{dom}f = \{x|c^Tx+d>0\|$. The image of $S$ under linear fractional functions is convex.
:::
:::

In some cases, the restrictions of **interior** is too strict. For example, imagine a plane in $\mathbb{R}^3$. The interior of the plane is $\emptyset$. But intuitively many property should be extended to this kind of situation. Because the points in the plane also lies 'inside' the convex set. Thus, we will define **relative interior**. First we will define **affine hull**.

::: {.definitionbox}
::: {.definition #affinehull name="Affine hull"}
  The affine hull of a set $S$ is the smallest affine set that contains $S$, which can be written as:
  $$\text{aff}(S) = \{\sum_{i=1}^k\alpha_ix_i|k>0,x_i\in S,\alpha_i\in\mathbb{R},\sum_{i=1}^k\alpha_i=1\}$$
:::
:::

::: {.definitionbox}
::: {.definition #relint name="Relative Interior"}
  The relative interior of a set $S$ (denoted $\text{relint}(S)$) is defined as its interior within the affine hull of $S$. I.e.
  $$\text{relint}(S):=\{x\in S: \text{there exists } \epsilon>0 \text{ such that }N_\epsilon \cap \text{aff}(S)\subset S\}$$
  where $N_\epsilon(x)$ is a ball of radius $\epsilon$ centered on $x$.
:::
:::

### Convex function {#appconvex-theory-convexfunction}

In this section, let's define convex functions:

::: {.definitionbox}
::: {.definition #defcvxfunc name="Convex function"}
  A function $f:\mathbb{R}^n\to\mathbb{R}$ is **convex** if $\textbf{dom}\ f$ is convex and $\forall x,y\in \textbf{dom}\ f$ and with $\theta \in [0,1]$, we have:$$f(\theta x +(1-\theta)y)\leq \theta f(x) + (1-\theta)f(y)$$
  The function is **strictly convex** if the inequality holds whenever $x\neq y$ and $\theta\in (0,1)$.
:::
:::

If a function is differentiable, it will be easier for us to check its convexity:

::: {.theorembox}
::: {.proposition #decidecvx name="Conditions for Convex function"}

  1.(First order condition) Suppose $f$ is differentiable, then $f$ is convex if and only if $\textbf{dom} f$ is convex and $\forall x,y\in \textbf{dom} f$, $$f(y)\geq f(x) +\nabla f(x)^T(y-x)$$
  2.(Second order conditions) Suppose $f$ is twice differentiable, then $f$ is convex if and only if $\textbf{dom} f$ is convex and $\forall x\in \textbf{dom} f$, $$\nabla^2 f(x) \succeq \textbf{0}$$

:::
:::

For the same purpose, some operations that preserve the convexity of the convex functions are presented here:

::: {.theorembox}
::: {.proposition #preservecvx name="Operations that preserve convexity"}
  Let $f:\mathbb{R}^n\to\mathbb{R}$ be a convex function and $g_1,...,g_n$ be convex functions. The following operations will preserve convexity of the function:
  
  1.(Nonnegative weighted sum): A nonnegative weighted sum of convex functions: $$f = \omega_1f_1 + ... +\omega_mf_m$$
  
  2.(Composition with an affine mapping) Suppose $A\in \mathbb{R}^{n\times m}$ and $b\in \mathbb{R}^n$, then $g(x) = f(Ax+b)$ is convex.
  
  3.(Pointwise maximum and supremum) $g(x) = \max\{g_1(x),...,g_n(x)\}$ is convex. If $h(x,y)$ is convex in $x$ for each $y\in\mathcal{A}$, then $\sup_{y\in\mathcal{A}} h(x,y)$ is also convex in $x$.
  
  4.(Minimization) If $h(x,y)$ is convex in $(x,y)$, and $C$ is a convex nonempty set, then $\inf_{x\in C} h(x,y)$ is convex in $x$.
  
  5.(Perspective of a function) The perspective of $f$ is the function $h:\mathbb{R}^{n+1}\to\mathbb{R}$ defined by: $h(x,t) = tf(x/t)$ with domain $\textbf{dom}\ h=\{(x,t)|x/t\in\textbf{dom} f,t>0\}$. And $h$ is convex.

:::
:::


### Lagrange dual

We consider an optimization problem in the standard form (without assuming convexity of anything):
\begin{equation}
\begin{aligned}
p^* = \quad \min_{x} \quad & f_0(x)\\
\textrm{s.t.} \quad & f_i(x)\leq 0\quad i=1...,m\\
  & h_i(x) = 0\quad i=1,...,p   \\
\end{aligned}
\end{equation}

::: {.definitionbox}
::: {.definition #defdualfunc name="Lagrange dual function"}
  The Lagrangian related to the problem above is defined as: $$L(x,\lambda,\nu)=f_0(x)+\sum_{i=1}^m\lambda_if_i(x)+\sum_{i=1}^p\nu_ih_i(x)$$
  The Lagrange dual function is defined as: $$g(\lambda,\nu) = \inf_{x\in\mathcal{D}}L(x,\lambda,\nu)$$
:::
:::

When the Lagrangian is unbounded below in $x$, the dual function takes on the value $-\infty$. Note that since the Lagrange dual function is a pointwise infimum of a family of affine functions of $(\lambda,\nu)$, so it's concave. The Lagrange dual function will give us lower bounds of the optimal value of the original problem: $$g(\lambda,\nu)\leq p^*$$. We can see that, the dual function can give a nontrivial lower bound only when $\lambda\succeq 0$. Thus we can solve the following dual problem to get the best lower bound.

::: {.definitionbox}
::: {.definition #defdualprob name="Lagrange dual problem"}
  The lagrangian dual problem is defined as follows:
  \begin{equation}
  \begin{aligned}
  d^* = \quad \max_{\lambda,\nu} \quad & g(\lambda,\nu)\\
  \textrm{s.t.} \quad & \lambda\succeq 0
  \end{aligned}
  \end{equation}
  This is a convex optimization problem.
:::
:::

We can easily see that $$d^*\leq p^*$$ always hold. This property is called **weak duality**. If $$d^*=p^*$$, it's called **strong duality**. Strong duality does not hold in general, but it usually holfs for convex problems. We can find conditions that guarantee strong duality in convex problems, which are called constrained qualifications. Slater's constraint qualification is a useful one.

::: {.theorembox}
::: {.theorem #slater name="Slater's constraint qualification"}
  Strong duality holds for a convex problem
  \begin{equation}
  \begin{aligned}
  p^* = \quad \min_{x} \quad & f_0(x)\\
  \textrm{s.t.} \quad & f_i(x)\leq 0\quad i=1...,m\\
    & Ax=b   \\
  \end{aligned}
  \end{equation}
  if it is strictly feasible, i.e.
  $$\exists x\in\textbf{relint}\mathcal{D}:\quad f_i(x)<0,\quad i=1...m,\quad Ax=b$$
  And the linear inequalities do not need to hold with strict inequality.
:::
:::

### KKT condition {#appconvex-theory-kkt}
KKT conditions.






# The Kalman-Yakubovich Lemma 

::: {.lemma #KalmanYakubovich name="Kalman-Yakubovich"}
Consider a controllable linear time-invariant system 
$$
\dot{x} = A x + b u \\
y = c^T x.
$$
The transfer function 
$$
h(p) = c^T (p I - A)^{-1} b 
$$
is strictly positive real (SPR) if and only if there exist positive definite matrices $P$ and $Q$ such that 
$$
A^T P + P A = - Q \\
Pb = c.
$$
:::



# Feedback Linearization {#feedbacklinearization}


# Sliding Control {#slidingcontrol}