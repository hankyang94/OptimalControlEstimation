# (APPENDIX) Appendix {-} 

# Linear System Theory

## Stability {#app-lti-stability}

Check this [note](https://intra.engr.ucr.edu/~enozari/teaching/ME120_Fall20/Lecture%203%20-%20Stability,%20Controllability%20&%20Observability.pdf). 

::: {.theorembox}
::: {.theorem #lyapunovequation name="Lyapunov Equation"}
The following is equivalent for a linear time-invariant system $\dot{x} = A x$

1. The system is globally asymptotically stable, i.e., $A$ is Hurwitz and $\lim_{t \rightarrow \infty} x(t) = 0$ regardless of the initial condition;

2. For any positive definite matrix $Q$, the unique solution $P$ to the Lyapunov equation 
\begin{equation}
A^T P + P A = -Q
(\#eq:lyapunov-equation)
\end{equation}
is positive definite.
:::
:::
::: {.proofbox}
::: {.proof}
(a): $2 \Rightarrow 1$. Suppose we are given two positive definie matrices $P, Q \succ 0$ that satisfies the Lyapunov equation \@ref(eq:lyapunov-equation). Define a scalar function 
$$
V(x) = x^T P x.
$$
It is clear that $V > 0$ for any $x \neq 0$ and $V(x) = 0$ (i.e., $V(x)$ is positive definie). We also see $V(x)$ is radially unbounded because:
$$
V(x) \geq \lambda_{\min}(P) \Vert x \Vert^2 \Rightarrow \lim_{x \rightarrow \infty} V(x) \rightarrow \infty.
$$
The time derivative of $V$ reads
$$
\dot{V} = 2 x^T P \dot{x} = x^T (A^T P + P A) x = - x^T Q x. 
$$
Clearly, $\dot{V} < 0$ for any $x \neq 0$ and $\dot{V}(0) = 0$. According to Lyapunov's global stability theorem \@ref(thm:lyapunovglobalstability), we conclude the linear system $\dot{x} = Ax$ is globally asymptotically stable at $x = 0$.

(b): $1 \Rightarrow 2$. Suppose $A$ is Hurwitz, we want to show that, for any $Q \succ 0$, there exists a unique $P \succ 0$ satisfying the Lyapunov equation \@ref(eq:lyapunov-equation). In fact, consider the matrix 
$$
P = \int_{t=0}^{\infty} e^{A^T t} Q e^{At} dt.
$$
Because $A$ is Hurwitz, the integral exists, and clearly $P \succ 0$ due to $Q \succ 0$. To show this choice of $P$ satisfies the Lyapunov equation, we write
\begin{align}
A^T P + P A &= \int_{t=0}^{\infty} \left( A^T e^{A^T t} Q e^{At} + e^{A^T t} Q e^{At} A  \right) dt \\
&=\int_{t=0}^{\infty} d \left( e^{A^T t} Q e^{At} \right) \\ 
& = e^{A^T t} Q e^{At}\vert_{t = \infty} - e^{A^T t} Q e^{At}\vert_{t = 0} = - Q,
\end{align}
where the last equality holds because $e^{A \infty} = 0$ (recall $A$ is Hurwitz).

To show the uniqueness of $P$, we assume that there exists another matrix $P'$ that also satisfies the Lyapunov equation. Therefore,
\begin{align}
P' &= e^{A^T t} P' e^{At} \vert_{t=0} - e^{A^T t} P' e^{At} \vert_{t=\infty} \\
 &= - \int_{t=0}^{\infty} d \left( e^{A^T t} P' e^{At} \right) \\
 &= - \int_{t=0}^{\infty} e^{A^T t} \left( A^T P' + P' A \right) e^{At} dt \\
 & = \int_{t=0}^{\infty} e^{A^T t} Q e^{At} dt = P,
\end{align}
leading to $P' = P$. Hence, the solution is unique. 
:::
:::

**Convergence rate estimation**. We now show that Theorem \@ref(thm:lyapunovequation) can allow us to quantify the convergence rate of a (stable) linear system towards zero. 

For a Hurwitz linear system $\dot{x} = Ax$, let us pick a positive definite matrix $Q$. Theorem \@ref(thm:lyapunovequation) tells us we can find a unique $P \succ 0$ satisfying the Lyapunov equation \@ref(eq:lyapunov-equation). In this case, we can upper bound the scalar function $V = x^T P x$ as 
$$
V \leq \lambda_{\max}(P) \Vert x \Vert^2.
$$
The time derivative of $V$ is $\dot{V} = - x^T Q x$, which can be upper bounded by 
\begin{align}
\dot{V} & \leq - \lambda_{\min} (Q) \Vert x \Vert^2 \\
& = - \frac{\lambda_{\min} (Q)}{\lambda_{\max} (P)} \underbrace{ \left( \lambda_{\max} (P) \Vert x \Vert^2 \right)}_{\geq V} \\
& \leq - \frac{\lambda_{\min} (Q)}{\lambda_{\max} (P)} V.
\end{align}
Denoting $\gamma(Q) = \frac{\lambda_{\min} (Q)}{\lambda_{\max}(P)}$, the above inequality implies 
$$
V(0) e^{-\gamma(Q) t} \geq V(t) = x^T P x \geq \lambda_{\min}(P) \Vert x \Vert^2.
$$
As a result, $\Vert x \Vert^2$ converges to zero exponentially with a rate at least $\gamma(Q)$, and $\Vert x \Vert$ converges to zero exponentially with a rate at least $\gamma(Q) / 2$.

**Best convergence rate estimation**. I have used $\gamma (Q)$ to make it explict that the rate $\gamma$ depends on the choice of $Q$, because $P$ is computed from the Lyapunov equation as an implicit function of $Q$. Naturally, choosing different $Q$ will lead to different $\gamma (Q)$. So what is the choice of $Q$ that maximizes the convergence rate estimation? 

::: {.corollary #bestconvergencerate name="Maximum Convergence Rate Estimation"}
$Q = I$ maximizes the convergence rate estimation.
:::
::: {.proofbox}
::: {.proof}
let us denote $P_0$ as the solution to the Lyapunov equation with $Q = I$
$$
A^T P_0 + P_0 A = - I.
$$
Let $P$ be the solution corresponding to a different choice of $Q$
$$
A^T P + P A = - Q.
$$
Without loss of generality, we can assume $\lambda_{\min}(Q) = 1$, because rescaling $Q$ will recale $P$ by the same factor, which does not affect $\gamma(Q)$. Subtracting the two Lyapunov equations above we get 
$$
A^T (P - P_0) + (P - P_0) A = - (Q - I).
$$
Since $Q - I \succeq 0$ (due to $\lambda_{\min}(Q) = 1$), we know $P - P_0 \succeq 0$ and $\lambda_{\max} (P) \geq \lambda_{\max} (P_0)$. As a result, 
$$
\gamma(Q) = \frac{\lambda_{\min}(Q)}{\lambda_{\max}(P)} =  \frac{\lambda_{\min}(I)}{\lambda_{\max}(P)} \leq \frac{\lambda_{\min}(I)}{\lambda_{\max}(P_0)} = \gamma(I),
$$
and $Q = I$ maximizes the convergence rate estimation.
:::
:::

---
Consider the linear time-invariant (LTI) system 
\begin{equation}
\dot{x} = A x + B u, \quad y = C x + D u,
(\#eq:app-linear-system)
\end{equation}
where $x \in \mathbb{R}^n$ the state, $u \in \mathbb{R}^m$ the control, and $A,B,C,D$ are constant matrices with proper sizes. 

::: {.theorembox}
::: {.theorem #ltiobservable name="Observability"}
The following are equivalent for the LTI system \@ref(eq:app-linear-system):

1. $(A,C)$ is observable;

2. The observability grammian
$$
W_o (t) = \int_{0}^t e^{A^* \tau} C^* C e^{A\tau} d\tau
$$
is positive definite for any $t > 0$;

3. The observability matrix 
$$
\mathcal{O} = \begin{bmatrix} C \\ CA \\ CA^2 \\ \vdots \\ C A^{n-1} \end{bmatrix}
$$
has full column rank;

4. The matrix $\begin{bmatrix} A - \lambda I \\ C \end{bmatrix}$ has full column rank for all $\lambda \in \mathbb{C}$;

5. For all $\lambda$ and $x$ such that $Ax = \lambda x$, $Cx \neq 0$;

6. The eigenvalues of $A + LC$ can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs);

7. $(A^*,C^*)$ is controllable.
:::
:::

::: {.theorembox}
::: {.theorem #ltidetectable name="Detectability"}
The following are equivalent for the LTI system \@ref(eq:app-linear-system):

1. $(A,C)$ is detectable;

2. The matrix $\begin{bmatrix} A - \lambda I \\ C \end{bmatrix}$ has full column rank for all $\lambda \in \mathbb{C}$ such that $\mathrm{Re}(\lambda) \geq 0$ ;

3. For all $\lambda$ and $x$ such that $Ax = \lambda x$ and $\mathrm{Re}(\lambda) \geq 0$, $Cx \neq 0$;

4. There exists a matrix $L$ such that $A + LC$ is Hurwitz;

5. $(A^*,C^*)$ is stabilizable.
:::
:::


# Convex Analysis and Optimization {#appconvex}

# The Kalman-Yakubovich Lemma 

::: {.lemma #KalmanYakubovich name="Kalman-Yakubovich"}
Consider a controllable linear time-invariant system 
$$
\dot{x} = A x + b u \\
y = c^T x.
$$
The transfer function 
$$
h(p) = c^T (p I - A)^{-1} b 
$$
is strictly positive real (SPR) if and only if there exist positive definite matrices $P$ and $Q$ such that 
$$
A^T P + P A = - Q \\
Pb = c.
$$
:::



# Feedback Linearization {#feedbacklinearization}


# Sliding Control {#slidingcontrol}

