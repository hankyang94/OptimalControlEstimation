[["index.html", "Optimal Control and Estimation Preface", " Optimal Control and Estimation Heng Yang 2023-06-11 Preface This is the textbook for Harvard ES/AM 158: Introduction to Optimal Control and Estimation. Information about the offerings of the class is listed below. 2023 Fall Time: Mon/Wed 2:15 - 3:30pm Location: Science and Engineering Complex, Room TBD Instructor: Heng Yang Teaching Fellow: Weiyu Li Syllabus Acknowledgment "],["formulation.html", "Chapter 1 The Optimal Control Formulation 1.1 The Basic Problem 1.2 Open-Loop v.s. Closed-Loop", " Chapter 1 The Optimal Control Formulation 1.1 The Basic Problem Consider a discrete-time dynamical system \\[\\begin{equation} x_{k+1} = f_k (x_k, u_k, w_k), \\quad k =0,1,\\dots,N-1 \\tag{1.1} \\end{equation}\\] where \\(x_k \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\) is the state of the system, \\(u_k \\in \\mathbb{U} \\subseteq \\mathbb{R}^m\\) is the control we wish to design, \\(w_k \\in \\mathbb{W} \\subseteq \\mathbb{R}^p\\) a random disturbance or noise (e.g., due to unmodelled dynamics) which is described by a probability distribution \\(P_k(\\cdot \\mid x_k, u_k)\\) that may depend on \\(x_k\\) and \\(u_k\\) but not on prior disturbances \\(w_0,\\dots,w_{k-1}\\), \\(k\\) indexes the discrete time, \\(N\\) denotes the horizon, \\(f_k\\) models the transition function of the system (typically \\(f_k \\equiv f\\) is time-invariant, especially for robotics systems; we use \\(f_k\\) here to keep full generality). Remark (Deterministic v.s. Stochastic). When \\(w_k \\equiv 0\\) for all \\(k\\), we say the system (1.1) is deterministic; otherwise we say the system is stochastic. In the following we will deal with the stochastic case, but most of the methodology should carry over to the deterministic setup. We consider the class of controllers (also called policies) that consist of a sequence of functions \\[ \\pi = \\{ \\mu_0,\\dots,\\mu_{N-1} \\}, \\] where \\(\\mu_k (x_k) \\in \\mathbb{U}\\) for all \\(x_k\\), i.e., \\(\\mu_k\\) is a feedback controller that maps the state to an admissible control. Given an initial state \\(x_0\\) and an admissible policy \\(\\pi\\), the state trajectory of the system is a sequence of random variables that evolve according to \\[\\begin{equation} x_{k+1} = f_k(x_k,\\mu_k(x_k),w_k), \\quad k=0,\\dots,N-1 \\tag{1.2} \\end{equation}\\] where the randomness comes from the disturbance \\(w_k\\). We assume the state-control trajectory \\(\\{u_k\\}_{k=0}^{N-1}\\) and \\(\\{x_k \\}_{k=0}^{N}\\) induce an additive cost \\[\\begin{equation} g_N(x_N) + \\sum_{k=0}^{N-1} g_k(x_k,u_k) \\tag{1.3} \\end{equation}\\] where \\(g_k,k=0,\\dots,N\\) are some user-designed functions. With (1.2) and (1.3), for any admissible policy \\(\\pi\\), we denote its induced expected cost with initial state \\(x_0\\) as \\[\\begin{equation} J_\\pi (x_0) = \\mathbb{E} \\left\\{ g_N(x_N) + \\sum_{k=0}^{N-1} g_k (x_k, \\mu_k(x_k)) \\right\\}, \\tag{1.4} \\end{equation}\\] where the expectation is taken over the randomness of \\(w_k\\). Definition 1.1 (Discrete-time, Finite-horizon Optimal Control) Find the best admissible controller that minimizes the expected cost in (1.4) \\[\\begin{equation} \\pi^\\star \\in \\arg\\min_{\\pi \\in \\Pi} J_\\pi(x_0), \\end{equation}\\] where \\(\\Pi\\) is the set of all admissible controllers. The cost attained by the optimal controller, i.e., \\(J^\\star = J_{\\pi^\\star}(x_0)\\) is called the optimal cost-to-go, or the optimal value function. 1.2 Open-Loop v.s. Closed-Loop An important feature of the basic problem in Definition 1.1 "],["stability.html", "Chapter 2 Stability Analysis", " Chapter 2 Stability Analysis Lemma 2.1 (Barbalat's Lemma) Let \\(f(t)\\) be differentiable, if \\(\\lim_{t \\rightarrow \\infty} f(t)\\) is finite, and \\(\\dot{f}(t)\\) is uniformly continuous,1 then \\[ \\lim_{t \\rightarrow \\infty} \\dot{f}(t) = 0. \\] Theorem 2.1 (Barbalat's Stability Certificate) If a scalar function \\(V(x,t)\\) satisfies \\(V(x,t)\\) is lower bounded, \\(\\dot{V}(x,t)\\) is negative semidefinite \\(\\dot{V}(x,t)\\) is uniformly continuous then \\(\\dot{V}(x,t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). Proof. \\(V(x,t)\\) is lower bounded and \\(\\dot{V}\\) is negative semidefinite implies the limit of \\(V\\) as \\(t \\rightarrow \\infty\\) is finite (note that \\(V(x,t) \\leq V(x(0),0)\\)). Then the theorem clearly follows from Barbalat’s Lemma 2.1. A sufficient condition for this to hold is that \\(\\ddot{f}\\) exists and is bounded.↩︎ "],["references.html", "References", " References "],["adaptivecontrol.html", "Chapter 3 Adaptive Control 3.1 First-Order Systems", " Chapter 3 Adaptive Control Lemma 3.1 (Basic Lemma) Let two signals \\(e(t)\\) and \\(\\phi(t)\\) be related by \\[\\begin{equation} e(t) = H(p)[k \\phi(t)^T v(t)] \\tag{3.1} \\end{equation}\\] where \\(e(t)\\) a scalar output signal, \\(H(p)\\) a strictly positive real (SPR) transfer function, \\(k\\) an unknown real number with known sign, \\(\\phi(t) \\in \\mathbb{R}^m\\) a control signal, and \\(v(t) \\in \\mathbb{R}^m\\) a measurable input signal. If the control signal \\(\\phi(t)\\) satisfies \\[\\begin{equation} \\dot{\\phi}(t) = - \\mathrm{sgn}(k) \\gamma e(t) v(t) \\tag{3.2} \\end{equation}\\] with \\(\\gamma &gt; 0\\) a positive constant, then \\(e(t)\\) and \\(\\phi(t)\\) are globally bounded. Moreover, if \\(v(t)\\) is bounded, then \\[ \\lim_{t \\rightarrow \\infty} e(t) = 0. \\] Proof. Let the state-space representation of (3.1) be \\[\\begin{equation} \\dot{x} = A x + b [k \\phi^T v], \\quad e = c^T x. \\tag{3.3} \\end{equation}\\] Since \\(H(p)\\) is SPR, it follows from the Kalman-Yakubovich Lemma A.1 that there exist \\(P,Q \\succ 0\\) such that \\[ A^T P + P A = -Q, \\quad Pb = c. \\] Let \\[ V(x,\\phi) = x^T P x + \\frac{|k|}{\\gamma} \\phi^T \\phi, \\] clearly V is positive definite (i.e., \\(V(0,0)=0\\), and \\(V(x,\\phi) &gt; 0\\) for all \\(x \\neq 0, \\phi \\neq 0\\)). The time derivative of \\(V\\) along the trajectory defined by (3.3) with \\(\\phi\\) chosen as in (3.2) is \\[\\begin{align} \\dot{V} &amp; = \\frac{\\partial V}{\\partial x} \\dot{x} + \\frac{\\partial V}{\\partial \\phi} \\dot{\\phi} \\\\ &amp;= x^T (PA + A^T P) x + 2 x^T P b (k \\phi^T v) + \\frac{2|k|}{\\gamma} \\phi^T (- \\mathrm{sgn}(k) \\gamma e v) \\\\ &amp; = - x^T Q x + 2 (x^T c)(k\\phi^T v) - 2 \\phi^T (e v) \\\\ &amp; = - x^T Q x \\leq 0. \\end{align}\\] As a result, we know \\(x\\) and \\(\\phi\\) must be bounded (\\(V(x(t),\\phi(t)) \\leq V(x(0),\\phi(0))\\) is bounded). Since \\(e = c^T x\\), we know \\(e\\) must be bounded as well. If the input signal \\(v\\) is also bounded, then \\(\\dot{x}\\) is bounded as seen from (3.3). Because \\(\\ddot{V} = -2x^T Q \\dot{x}\\) is now bounded, we know \\(\\dot{V}\\) is uniformly continuous. Therefore, by Barbalat’s stability certificate (Theorem 2.1), we know \\(\\dot{V}\\) tends to zero as \\(t\\) tends to infinity, which implies \\(\\lim_{t \\rightarrow \\infty} x(t) = 0\\) and hence \\(\\lim_{t \\rightarrow \\infty} e(t) = 0\\). 3.1 First-Order Systems 3.1.1 Linear Systems Consider the first-order single-input single-output (SISO) system \\[\\begin{equation} \\dot{x} = - a x + b u \\tag{3.4} \\end{equation}\\] where \\(a\\) and \\(b\\) are unknown groundtruth parameters. However, we do assume that the sign of \\(b\\) is known. What if the sign of \\(b\\) is unknown too? Let \\(r(t)\\) be a reference trajectory, e.g., a step function or a sinusoidal function, and \\(x_d(t)\\) be a desired system trajectory that tracks the reference \\[\\begin{equation} \\dot{x}_d = - a_d x_d + b_d r(t), \\tag{3.5} \\end{equation}\\] where \\(a_d,b_d &gt; 0\\) are user-defined constants. Note that the transfer function from \\(r\\) to \\(x_d\\) is \\[ x_d = \\frac{b_d}{p + a_d} r \\] and the system is stable. Review basics of transfer function. The goal of adaptive control is to design a control law and an adaptation law such that the tracking error of the system \\(x(t) - x_d(t)\\) converges to zero. Control law. We design the control law as \\[\\begin{equation} u = \\hat{a}_r(t) r + \\hat{a}_x(t) x \\tag{3.6} \\end{equation}\\] where \\(\\hat{a}_r(t)\\) and \\(\\hat{a}_x(t)\\) are time-varying feedback gains that we wish to adapt. The closed-loop dynamics of system (3.4) with the controller (3.6) is \\[ \\dot{x} = - a x + b (\\hat{a}_r r + \\hat{a}_x x) = - (a - b \\hat{a}_x) x + b \\hat{a}_r r. \\] With the equation above, the reason for choosing the control law (3.6) is clear: if the system parameters \\((a,b)\\) were known, then choosing \\[\\begin{equation} a_r^\\star = \\frac{b_d}{b}, \\quad a_x^\\star = \\frac{a - a_d}{b} \\tag{3.7} \\end{equation}\\] leads to the closed-loop dynamics \\(\\dot{x} = - a_d x + b_d r\\) that is exactly what we want in (3.5). However, in adaptive control, since the true parameters \\((a,b)\\) are not revealed to the control designer, an adaptation law is needed to dynamically adjust the gains \\(\\hat{a}_r\\) and \\(\\hat{a}_x\\) based on the tracking error \\(x(t) - x_d(t)\\). Adaptation law. Let \\(e(t) = x(t) - x_d(t)\\) be the tracking error, and we develop its time derivative \\[\\begin{align} \\dot{e} &amp;= \\dot{x} - \\dot{x}_d \\\\ &amp;= - a_d (x - x_d) + (a_d - a + b\\hat{a}_x)x + (b \\hat{a}_r - b_d) r \\\\ &amp; = - a_d e + b\\underbrace{(\\hat{a}_x - \\hat{a}_x^\\star)}_{=:\\tilde{a}_x} x + b \\underbrace{(\\hat{a}_r - \\hat{a}_r^\\star )}_{=:\\tilde{a}_r} r \\\\ &amp; = - a_d e + b (\\tilde{a}_x x + \\tilde{a}_r r) \\tag{3.8} \\end{align}\\] where \\(\\tilde{a}_x\\) and \\(\\tilde{a}_r\\) are the gain errors w.r.t. the optimal gains in (3.7) if the true parameters were known. The error dynamics (3.8) is equivalent to the following transfer function \\[ e = \\frac{1}{p + a_d} b(\\tilde{a}_x x + \\tilde{a}_r r) = \\frac{1}{p + a_d} \\left(b \\begin{bmatrix} \\tilde{a}_x \\\\ \\tilde{a}_r \\end{bmatrix}^T \\begin{bmatrix} x \\\\ r \\end{bmatrix} \\right), \\] which is in the form of (3.1). Therefore, we choose the adaptation law \\[\\begin{equation} \\begin{bmatrix} \\dot{\\tilde{a}}_x \\\\ \\dot{\\tilde{a}}_r \\end{bmatrix} = - \\mathrm{sgn}(b) \\gamma e \\begin{bmatrix} x \\\\ r \\end{bmatrix}. \\tag{3.9} \\end{equation}\\] Tracking convergence. With the control law (3.6) and the adaptation law (3.9), we can prove that the tracking error converges to zero, using Lemma 3.1. With \\(\\tilde{a}=[\\tilde{a}_x, \\tilde{a}_r]^T\\), let \\[ V(e,\\tilde{a}) = e^2 + \\frac{|b|}{\\gamma} \\tilde{a}^T \\tilde{a} \\] be a positive definite Lyapunov function candidate with time derivative \\[ \\dot{V} = - 2a_d e^2 \\leq 0. \\] Clearly, \\(e\\) and \\(\\tilde{a}\\) are both bounded. Assuming the reference trajectory \\(r\\) is bounded, we know \\(x_d\\) is bounded (due to (3.5)) and hence \\(x\\) is bounded (due to \\(e = x - x_d\\) being bounded). Consequently, from the error dynamics (3.8) we know \\(\\dot{e}\\) is bounded, which implies \\(\\ddot{V} = -4a_d e \\dot{e}\\) is bounded and \\(\\dot{V}\\) is uniformly continuous. By Barbalat’s stability certificate 2.1, we conlude \\(e(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). 3.1.2 Nonlinear Systems "],["the-kalman-yakubovich-lemma.html", "A The Kalman-Yakubovich Lemma", " A The Kalman-Yakubovich Lemma Lemma A.1 (Kalman-Yakubovich) Consider a controllable linear time-invariant system \\[ \\dot{x} = A x + b u \\\\ y = c^T x. \\] The transfer function \\[ h(p) = c^T (p I - A)^{-1} b \\] is strictly positive real (SPR) if and only if there exist positive definite matrices \\(P\\) and \\(Q\\) such that \\[ A^T P + P A = - Q \\\\ Pb = c. \\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
